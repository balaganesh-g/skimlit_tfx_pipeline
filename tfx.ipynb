{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLCbiPmo9THF",
        "outputId": "41b3edb0-3c98-4c3e-e142-09ea88349d7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tfx in /usr/local/lib/python3.8/dist-packages (1.12.0)\n",
            "Requirement already satisfied: google-apitools<1,>=0.5 in /usr/local/lib/python3.8/dist-packages (from tfx) (0.5.31)\n",
            "Requirement already satisfied: google-api-python-client<2,>=1.8 in /usr/local/lib/python3.8/dist-packages (from tfx) (1.12.11)\n",
            "Requirement already satisfied: protobuf<4,>=3.13 in /usr/local/lib/python3.8/dist-packages (from tfx) (3.19.6)\n",
            "Requirement already satisfied: jinja2<4,>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from tfx) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=3.10.0.2 in /usr/local/lib/python3.8/dist-packages (from tfx) (4.4.0)\n",
            "Requirement already satisfied: apache-beam[gcp]<3,>=2.40 in /usr/local/lib/python3.8/dist-packages (from tfx) (2.43.0)\n",
            "Requirement already satisfied: numpy<2,>=1.16 in /usr/local/lib/python3.8/dist-packages (from tfx) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-data-validation<1.13.0,>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tfx) (1.12.0)\n",
            "Requirement already satisfied: ml-pipelines-sdk==1.12.0 in /usr/local/lib/python3.8/dist-packages (from tfx) (1.12.0)\n",
            "Requirement already satisfied: docker<5,>=4.1 in /usr/local/lib/python3.8/dist-packages (from tfx) (4.4.4)\n",
            "Requirement already satisfied: attrs<22,>=19.3.0 in /usr/local/lib/python3.8/dist-packages (from tfx) (21.4.0)\n",
            "Requirement already satisfied: ml-metadata<1.13.0,>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tfx) (1.12.0)\n",
            "Requirement already satisfied: keras-tuner<2,>=1.0.4 in /usr/local/lib/python3.8/dist-packages (from tfx) (1.1.3)\n",
            "Requirement already satisfied: google-cloud-aiplatform<1.18,>=1.6.2 in /usr/local/lib/python3.8/dist-packages (from tfx) (1.17.1)\n",
            "Requirement already satisfied: packaging<21,>=20 in /usr/local/lib/python3.8/dist-packages (from tfx) (20.9)\n",
            "Requirement already satisfied: tensorflow-hub<0.13,>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from tfx) (0.12.0)\n",
            "Requirement already satisfied: google-api-core<1.33 in /usr/local/lib/python3.8/dist-packages (from tfx) (1.32.0)\n",
            "Requirement already satisfied: grpcio<2,>=1.28.1 in /usr/local/lib/python3.8/dist-packages (from tfx) (1.51.1)\n",
            "Requirement already satisfied: pyyaml<6,>=3.12 in /usr/local/lib/python3.8/dist-packages (from tfx) (5.4.1)\n",
            "Requirement already satisfied: google-cloud-bigquery<3,>=2.26.0 in /usr/local/lib/python3.8/dist-packages (from tfx) (2.34.4)\n",
            "Requirement already satisfied: tensorflow-transform<1.13.0,>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tfx) (1.12.0)\n",
            "Requirement already satisfied: pyarrow<7,>=6 in /usr/local/lib/python3.8/dist-packages (from tfx) (6.0.1)\n",
            "Requirement already satisfied: portpicker<2,>=1.3.1 in /usr/local/lib/python3.8/dist-packages (from tfx) (1.3.9)\n",
            "Requirement already satisfied: click<8,>=7 in /usr/local/lib/python3.8/dist-packages (from tfx) (7.1.2)\n",
            "Requirement already satisfied: kubernetes<13,>=10.0.1 in /usr/local/lib/python3.8/dist-packages (from tfx) (12.0.1)\n",
            "Requirement already satisfied: tensorflow-model-analysis<0.44.0,>=0.43.0 in /usr/local/lib/python3.8/dist-packages (from tfx) (0.43.0)\n",
            "Requirement already satisfied: tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15 in /usr/local/lib/python3.8/dist-packages (from tfx) (2.11.0)\n",
            "Requirement already satisfied: tensorflow<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tfx) (2.11.0)\n",
            "Requirement already satisfied: tfx-bsl<1.13.0,>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tfx) (1.12.0)\n",
            "Requirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.8/dist-packages (from tfx) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2.8.2)\n",
            "Requirement already satisfied: cloudpickle~=2.2.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2.2.0)\n",
            "Requirement already satisfied: httplib2<0.21.0,>=0.8 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (0.17.4)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.22.1)\n",
            "Requirement already satisfied: fasteners<1.0,>=0.3 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (0.18)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2022.6.2)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2.7.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2.28.1)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (0.19.0)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.3.0)\n",
            "Requirement already satisfied: fastavro<2,>=0.23.6 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.7.0)\n",
            "Requirement already satisfied: objsize<0.6.0,>=0.5.2 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (0.5.2)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.7)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (0.3.1.1)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (3.13.0)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2022.6)\n",
            "Requirement already satisfied: orjson<4.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (3.8.3)\n",
            "Requirement already satisfied: cachetools<5,>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (4.2.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.35.0)\n",
            "Requirement already satisfied: google-cloud-videointelligence<2,>=1.8.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.16.3)\n",
            "Requirement already satisfied: google-cloud-language<2,>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.3.2)\n",
            "Requirement already satisfied: google-cloud-recommendations-ai<0.8.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (0.7.1)\n",
            "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.6.0)\n",
            "Requirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (0.1.0)\n",
            "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (3.9.2)\n",
            "Requirement already satisfied: google-cloud-vision<2,>=0.38.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.0.2)\n",
            "Requirement already satisfied: google-cloud-bigtable<2,>=0.31.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.7.3)\n",
            "Requirement already satisfied: google-cloud-datastore<2,>=1.8.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (1.15.5)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage<2.14,>=2.6.3 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2.13.2)\n",
            "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2.13.11)\n",
            "Requirement already satisfied: google-cloud-core<3,>=0.28.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (2.3.2)\n",
            "Requirement already satisfied: google-cloud-spanner<4,>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]<3,>=2.40->tfx) (3.26.0)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.8/dist-packages (from docker<5,>=4.1->tfx) (1.4.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker<5,>=4.1->tfx) (1.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from google-api-core<1.33->tfx) (1.57.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.8/dist-packages (from google-api-core<1.33->tfx) (57.4.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client<2,>=1.8->tfx) (3.0.1)\n",
            "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.8/dist-packages (from google-apitools<1,>=0.5->tfx) (4.1.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.40->tfx) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.40->tfx) (0.2.8)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform<1.18,>=1.6.2->tfx) (2.5.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform<1.18,>=1.6.2->tfx) (1.6.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<3,>=2.26.0->tfx) (2.4.0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]<3,>=2.40->tfx) (0.12.4)\n",
            "Requirement already satisfied: grpcio-status>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.40->tfx) (1.48.2)\n",
            "Requirement already satisfied: overrides<7.0.0,>=6.0.1 in /usr/local/lib/python3.8/dist-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.40->tfx) (6.5.0)\n",
            "Requirement already satisfied: sqlparse>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.40->tfx) (0.4.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.8/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3,>=2.26.0->tfx) (1.5.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.8/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.40->tfx) (0.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2<4,>=2.7.3->tfx) (2.0.1)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.8/dist-packages (from keras-tuner<2,>=1.0.4->tfx) (1.0.4)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from keras-tuner<2,>=1.0.4->tfx) (7.9.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (from keras-tuner<2,>=1.0.4->tfx) (2.11.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.8/dist-packages (from kubernetes<13,>=10.0.1->tfx) (2022.12.7)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.8/dist-packages (from kubernetes<13,>=10.0.1->tfx) (1.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.8/dist-packages (from kubernetes<13,>=10.0.1->tfx) (1.24.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from oauth2client>=1.4.12->google-apitools<1,>=0.5->tfx) (0.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging<21,>=20->tfx) (3.0.9)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.40->tfx) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.40->tfx) (2.10)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tfx) (0.28.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tfx) (22.12.6)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tfx) (3.1.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tfx) (14.0.6)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tfx) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tfx) (2.1.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tfx) (0.2.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tfx) (2.11.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tfx) (2.11.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tfx) (3.3.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tfx) (0.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tfx) (1.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow<2.12,>=2.11.0->tfx) (0.38.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx) (0.4.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner<2,>=1.0.4->tfx) (5.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner<2,>=1.0.4->tfx) (3.11.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib->kubernetes<13,>=10.0.1->tfx) (3.2.2)\n",
            "Requirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-data-validation<1.13.0,>=1.12.0->tfx) (1.3.5)\n",
            "Requirement already satisfied: pyfarmhash<0.4,>=0.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-data-validation<1.13.0,>=1.12.0->tfx) (0.3.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-data-validation<1.13.0,>=1.12.0->tfx) (1.2.0)\n",
            "Requirement already satisfied: tensorflow-metadata<1.13,>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-data-validation<1.13.0,>=1.12.0->tfx) (1.12.0)\n",
            "Requirement already satisfied: scipy<2,>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (1.7.3)\n",
            "Requirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (7.7.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner<2,>=1.0.4->tfx) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner<2,>=1.0.4->tfx) (0.7.5)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner<2,>=1.0.4->tfx) (0.18.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner<2,>=1.0.4->tfx) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner<2,>=1.0.4->tfx) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner<2,>=1.0.4->tfx) (5.7.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner<2,>=1.0.4->tfx) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner<2,>=1.0.4->tfx) (2.0.10)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (3.6.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (3.0.4)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.2.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (5.3.4)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (6.0.4)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (6.1.12)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython->keras-tuner<2,>=1.0.4->tfx) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->keras-tuner<2,>=1.0.4->tfx) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.8/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (5.7.16)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (1.8.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (5.7.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.13.3)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (5.1.0)\n",
            "Requirement already satisfied: nbconvert<6.0 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (5.6.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.15.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (23.2.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core>=4.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (2.6.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (5.0.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (1.5.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.8.4)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (2.16.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (4.3.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.19.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (5.10.1)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.8/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.7.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.44.0,>=0.43.0->tfx) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tfx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLNsGPma_9nJ",
        "outputId": "288762e5-6674-4610-8f20-a4937b39036a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'pubmed-rct' already exists and is not an empty directory.\n",
            "PubMed_200k_RCT\n",
            "PubMed_200k_RCT_numbers_replaced_with_at_sign\n",
            "PubMed_20k_RCT\n",
            "PubMed_20k_RCT_numbers_replaced_with_at_sign\n",
            "README.md\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git\n",
        "!ls pubmed-rct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6oEZDpLN-Egs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import absl\n",
        "import tempfile\n",
        "import tensorflow as tf\n",
        "\n",
        "from tfx import v1 as tfx\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eElBB9E5-FD6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58aaa485-4689-49fa-9041-48265b2a3b7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.11.0\n",
            "TFX version: 1.12.0\n"
          ]
        }
      ],
      "source": [
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "print('TFX version: {}'.format(tfx.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xQKicOyK-FK7"
      },
      "outputs": [],
      "source": [
        "_tfx_root = tfx.__path__[0]\n",
        "_skimlit_root = os.path.join(_tfx_root, 'examples/skimlit_pipeline')\n",
        "\n",
        "# This is the path where your model will be pushed for serving.\n",
        "_serving_model_dir = os.path.join(\n",
        "    tempfile.mkdtemp(), 'serving_model/skimlit_simple')\n",
        "\n",
        "# Set up logging.\n",
        "absl.logging.set_verbosity(absl.logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lMaSncpM-FR3"
      },
      "outputs": [],
      "source": [
        "_data_root = tempfile.mkdtemp(prefix='tfx-data')\n",
        "DATA_PATH = '/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign'\n",
        "_data_filepath = os.path.join(_data_root, \"PubMed_20k_RCT_numbers_replaced_with_at_sign\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MMEqP1odSThk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "182aee1d-c2de-4acd-d7d6-bdae2a26c431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:InteractiveContext pipeline_root argument not provided: using temporary directory /tmp/tfx-interactive-2022-12-20T09_23_01.271488-_jrr3gqi as root for pipeline outputs.\n",
            "WARNING:absl:InteractiveContext metadata_connection_config not provided: using SQLite ML Metadata database at /tmp/tfx-interactive-2022-12-20T09_23_01.271488-_jrr3gqi/metadata.sqlite.\n"
          ]
        }
      ],
      "source": [
        "context = InteractiveContext()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1EQ4TQ4HkId-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f03e869d-c862-4fb2-e474-8ef8e0a0973f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting preprocessing.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile preprocessing.py\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    _data_root = os.mkdir('tfx-data')\n",
        "except FileExistsError:\n",
        "   # directory already exists\n",
        "   pass\n",
        "\n",
        "_data_root = os.getcwd()+'/tfx-data'\n",
        "data_dir = '/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign'\n",
        "\n",
        "def read_lines(filename):\n",
        "    \"\"\"args:\n",
        "            filename - name/path of the file \n",
        "       returns:\n",
        "            reads line by and make each sentence as separate line and return a list of strings\n",
        "    \"\"\"\n",
        "    with open(filename, 'r') as f:\n",
        "        return f.readlines()\n",
        "\n",
        "def preprocess_text_with_line_numbers(filename):\n",
        "    train_eg = read_lines(filename)\n",
        "    abstract_samples = []\n",
        "    abstract_lines = \"\"\n",
        "    for line in train_eg:\n",
        "        if line.startswith('###'):\n",
        "            abstract_id = line\n",
        "            abstract_lines = \"\"\n",
        "        elif line.isspace():\n",
        "            abstract_line_split = abstract_lines.splitlines()\n",
        "            \n",
        "            for abstract_line_number,abstract_line in enumerate(abstract_line_split):\n",
        "                line_data = {}\n",
        "                target_line_and_label = abstract_line.split('\\t')\n",
        "                line_data['target'] = target_line_and_label[0]\n",
        "                line_data['text'] = target_line_and_label[1] \n",
        "                line_data['line_number'] = abstract_line_number\n",
        "                line_data['total_lines'] = len(abstract_line_split)\n",
        "                abstract_samples.append(line_data)\n",
        "        else:\n",
        "            abstract_lines += line \n",
        "    return abstract_samples     \n",
        "\n",
        "train_samples = preprocess_text_with_line_numbers(data_dir +'/'+ \"train.txt\")\n",
        "val_sample = preprocess_text_with_line_numbers(data_dir+'/'+ \"dev.txt\")\n",
        "test_sample = preprocess_text_with_line_numbers(data_dir+'/'+ \"test.txt\")\n",
        "\n",
        "\n",
        "train_df = pd.DataFrame(train_samples)\n",
        "val_df = pd.DataFrame(val_sample)\n",
        "test_df = pd.DataFrame(test_sample)\n",
        "\n",
        "\n",
        "\n",
        "train_df['target_int'] = pd.Categorical(train_df['target']).codes\n",
        "val_df['target_int'] = pd.Categorical(val_df['target']).codes\n",
        "test_df['target_int'] = pd.Categorical(test_df['target']).codes\n",
        "\n",
        "\n",
        "train_df = train_df.to_csv(os.path.join(_data_root,\"train.csv\"))\n",
        "val_df = val_df.to_csv(os.path.join(_data_root,\"val.csv\"))\n",
        "test_df = test_df.to_csv(os.path.join(_data_root,\"test.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HuKOKNn009qP"
      },
      "outputs": [],
      "source": [
        "!python preprocessing.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0F2rZT4IkfxU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "dedcbaf0-90d9-42b2-86fe-0840ee5d3450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:Running driver for CsvExampleGen\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:select span and version = (0, None)\n",
            "INFO:absl:latest span and version = (0, None)\n",
            "INFO:absl:Running executor for CsvExampleGen\n",
            "INFO:absl:Generating examples.\n",
            "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:Processing input csv data /content/tfx-data/* to TFExample.\n",
            "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
          ]
        }
      ],
      "source": [
        "# Examplegen to ingest data into pipeline\n",
        "example_gen = tfx.components.CsvExampleGen(input_base='/content/tfx-data/')\n",
        "context.run(example_gen, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhbBS7mMkg-4"
      },
      "outputs": [],
      "source": [
        "import pprint \n",
        "pp = pprint.PrettyPrinter()\n",
        "\n",
        "train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri,'Split-train')\n",
        "tfrecord_filename = [os.path.join(train_uri,name)\n",
        "                       for name in os.listdir(train_uri) ]\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filename,compression_type='GZIP')\n",
        "\n",
        "\n",
        "for record in dataset.take(3):\n",
        "    serialized_example = record.numpy()\n",
        "    example = tf.train.Example()\n",
        "    example.ParseFromString(serialized_example)\n",
        "    pp.pprint(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXuRpUNrkiBH"
      },
      "outputs": [],
      "source": [
        "statistic_gen = tfx.components.StatisticsGen(examples=example_gen.outputs['examples'])\n",
        "context.run(statistic_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg3WcDClkicE"
      },
      "outputs": [],
      "source": [
        "context.show(statistic_gen.outputs['statistics'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8yUHLqhkijt"
      },
      "outputs": [],
      "source": [
        "schema_gen = tfx.components.SchemaGen(statistics=statistic_gen.outputs['statistics'],infer_feature_shape=False)\n",
        "context.run(schema_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDSiMtlfki0N"
      },
      "outputs": [],
      "source": [
        "context.show(schema_gen.outputs['schema'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74qCNGblki2e"
      },
      "outputs": [],
      "source": [
        "example_validator = tfx.components.ExampleValidator(\n",
        "    statistics=statistic_gen.outputs['statistics'],\n",
        "    schema=schema_gen.outputs['schema']\n",
        ")\n",
        "context.run(example_validator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KbMnByTkjNu"
      },
      "outputs": [],
      "source": [
        "context.show(example_validator.outputs['anomalies'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yFkHzUqEtvP"
      },
      "outputs": [],
      "source": [
        "%%writefile constant.py\n",
        "ONE_HOT_FEATURES = ['line_number','total_lines']\n",
        "CAT_FEATURES = 'text'\n",
        "TARGET_FEATURE = 'target_int'\n",
        "VOCAB_SIZE = 60000\n",
        "OOV_SIZE = 10\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Oc_4acKkjup"
      },
      "outputs": [],
      "source": [
        "%%writefile transform.py\n",
        "import sys\n",
        "import absl\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft \n",
        "import constant\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "\n",
        "if 'google.colab' in sys.modules:  # Testing to see if we're doing development\n",
        "  import importlib\n",
        "  importlib.reload(constant)\n",
        "\n",
        "\n",
        "_ONE_HOT_FEATURES = constant.ONE_HOT_FEATURES\n",
        "_CAT_FEATURES = constant.ONE_HOT_FEATURES\n",
        "_TARGET_FEATURE = constant.TARGET_FEATURE\n",
        "_OOV_SIZE = constant.OOV_SIZE\n",
        "_VOCAB_SIZE = constant.VOCAB_SIZE\n",
        "\n",
        "\n",
        "def _make_one_hot(x,key):\n",
        "\n",
        "  one_hot_encoded = tf.one_hot(\n",
        "      x,\n",
        "      depth=20,\n",
        "      on_value=1.0,\n",
        "      off_value=0.0)\n",
        "  return tf.reshape(one_hot_encoded,[-1,20])\n",
        "\n",
        "\n",
        "def _make_one_hot_line_number(x,key):\n",
        "\n",
        "  one_hot_encoded = tf.one_hot(\n",
        "      x,\n",
        "      depth=15,\n",
        "      on_value=1.0,\n",
        "      off_value=0.0)\n",
        "  return tf.reshape(one_hot_encoded,[-1,15])\n",
        "\n",
        "\n",
        "def _fill_in_missing(x):\n",
        "  \"\"\"Replace missing values in a SparseTensor.\n",
        "  Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n",
        "  Args:\n",
        "    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n",
        "      in the second dimension.\n",
        "  Returns:\n",
        "    A rank 1 tensor where missing values of `x` have been filled in.\n",
        "  \"\"\"\n",
        "  if not isinstance(x, tf.sparse.SparseTensor):\n",
        "    return x\n",
        "\n",
        "  default_value = '' if x.dtype == tf.string else 0\n",
        "  return tf.squeeze(\n",
        "      tf.sparse.to_dense(\n",
        "          tf.sparse.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
        "          default_value),\n",
        "      axis=1)\n",
        "\n",
        "\n",
        "def _make_train_char(x):\n",
        "    s = tf.strings.regex_replace(x, ' ', '')\n",
        "    s = tf.strings.regex_replace(s, '', ' ')\n",
        "    s = tf.strings.strip(s)\n",
        "    return x ,s\n",
        "\n",
        "\n",
        "def _make_one_hot_target(x):\n",
        "\n",
        "  one_hot_encoded = tf.one_hot(\n",
        "      x,\n",
        "      depth=5,\n",
        "      on_value=1.0,\n",
        "      off_value=0.0)\n",
        "  return tf.reshape(one_hot_encoded,[-1,5])\n",
        "\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "  \"\"\"tf.transform's callback function for preprocessing inputs.\n",
        "  Args:\n",
        "    inputs: map from feature keys to raw not-yet-transformed features.\n",
        "  Returns:\n",
        "    Map from string feature key to transformed feature operations.\n",
        "  \"\"\"\n",
        "  outputs = {}\n",
        "\n",
        "  total_lines = inputs['total_lines']\n",
        "  onehot_total_lines = _make_one_hot(_fill_in_missing(total_lines), 'total_lines')\n",
        "\n",
        "  line_number = inputs['line_number']\n",
        "  line_numbers = _make_one_hot_line_number(_fill_in_missing(line_number), 'line_number')\n",
        "\n",
        "  text = inputs['text']\n",
        "  train_sentence,train_chars = _make_train_char(_fill_in_missing(text))\n",
        "\n",
        "  target = inputs['target_int']\n",
        "  target = _make_one_hot_target(_fill_in_missing(target))\n",
        "\n",
        "  return  {\n",
        "      'line_number_input':line_numbers,\n",
        "      'total_lines_input':onehot_total_lines,\n",
        "      'token_inputs':train_sentence,\n",
        "      'char_inputs':  train_chars,\n",
        "      'target_int': target\n",
        "  }  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEFADjV4kkCB"
      },
      "outputs": [],
      "source": [
        "transform = tfx.components.Transform(\n",
        "    examples = example_gen.outputs['examples'],\n",
        "    schema = schema_gen.outputs['schema'],\n",
        "    module_file = os.path.abspath('transform.py'))\n",
        "context.run(transform, enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_uri = transform.outputs['transform_graph'].get()[0].uri\n",
        "os.listdir(train_uri)"
      ],
      "metadata": {
        "id": "K4mfTwHs-MGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint \n",
        "pp = pprint.PrettyPrinter()\n",
        "\n",
        "train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\n",
        "\n",
        "# Get the list of files in this directory (all compressed TFRecord files)\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                      for name in os.listdir(train_uri)]\n",
        "\n",
        "# Create a `TFRecordDataset` to read these files\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
        "\n",
        "# Iterate over the first 3 records and decode them.\n",
        "for tfrecord in dataset.take(3):\n",
        "  serialized_example = tfrecord.numpy()\n",
        "  example = tf.train.Example()\n",
        "  example.ParseFromString(serialized_example)\n",
        "  pp.pprint(example)"
      ],
      "metadata": {
        "id": "kURDT_i0-Mjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skimlit_trainer_module_file = 'skimlit_trainer.py'"
      ],
      "metadata": {
        "id": "xGcxfiem-MpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {skimlit_trainer_module_file}\n",
        "\n",
        "from typing import Dict, List, Text\n",
        "\n",
        "import os\n",
        "import glob\n",
        "from absl import logging\n",
        "\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "import tensorflow_hub as hub\n",
        "from tfx import v1 as tfx\n",
        "from tfx_bsl.public import tfxio\n",
        "from tensorflow_transform import TFTransformOutput\n",
        "import string\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
        "\n",
        "\n",
        "_LABEL_KEY = 'target_int'\n",
        "NUM_CHAR_TOKENS = len(alphabet) + 2\n",
        "output_seq_char_len= 290\n",
        "\n",
        "\n",
        "\n",
        "def _input_fn(file_pattern: List[Text],\n",
        "              data_accessor: tfx.components.DataAccessor,\n",
        "              tf_transform_output: tft.TFTransformOutput,\n",
        "              batch_size: int = 64) -> tf.data.Dataset:\n",
        "    \n",
        "    dataset_ = data_accessor.tf_dataset_factory(\n",
        "                                        file_pattern,\n",
        "                                        tfxio.TensorFlowDatasetOptions(batch_size=batch_size, label_key=_LABEL_KEY),\n",
        "                                        tf_transform_output.transformed_metadata.schema)\n",
        "    return dataset_   \n",
        "                                 \n",
        "    \n",
        "\n",
        "def _get_tf_examples_serving_signature(model, tf_transform_output):\n",
        "  \"\"\"Returns a serving signature that accepts `tensorflow.Example`.\"\"\"\n",
        "  # We need to track the layers in the model in order to save it.\n",
        "  # TODO(b/162357359): Revise once the bug is resolved.\n",
        "\n",
        "\n",
        "  model.tft_layer_inference = tf_transform_output.transform_features_layer()\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')])\n",
        "  def serve_tf_examples_fn(serialized_tf_example):\n",
        "    \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
        "\n",
        "    raw_feature_spec = tf_transform_output.raw_feature_spec()\n",
        " \n",
        "    # Remove label feature since these will not be present at serving time.\n",
        "    raw_feature_spec.pop(_LABEL_KEY)\n",
        "    raw_feature_spec.pop('target')\n",
        "    \n",
        "    raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n",
        "    transformed_features = model.tft_layer_inference(raw_features)\n",
        "    logging.info('serve_transformed_features = %s', transformed_features)\n",
        "\n",
        "    outputs = model(transformed_features)\n",
        "\n",
        "    # TODO(b/154085620): Convert the predicted labels from the model using a\n",
        "    # reverse-lookup (opposite of transform.py).\n",
        "\n",
        "    return {'outputs': outputs}\n",
        "\n",
        "  return serve_tf_examples_fn\n",
        "\n",
        "\n",
        "def _get_transform_features_signature(model, tf_transform_output):\n",
        "  \"\"\"Returns a serving signature that applies tf.Transform to features.\"\"\"\n",
        "  \n",
        "  # We need to track the layers in the model in order to save it.\n",
        "  # TODO(b/162357359): Revise once the bug is resolved.\n",
        "\n",
        "  model.tft_layer_eval = tf_transform_output.transform_features_layer()\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')])\n",
        "  def transform_features_fn(serialized_tf_example):\n",
        "    \"\"\"Returns the transformed_features to be fed as input to evaluator.\"\"\"\n",
        "\n",
        "    raw_feature_spec = tf_transform_output.raw_feature_spec()\n",
        "    raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n",
        "  \n",
        "    transformed_features = model.tft_layer_eval(raw_features)\n",
        "    logging.info('eval_transformed_features = %s', transformed_features)\n",
        "  \n",
        "    return transformed_features\n",
        "\n",
        "  return transform_features_fn\n",
        "\n",
        "\n",
        "def export_serving_model(tf_transform_output, model, output_dir):\n",
        "  \"\"\"Exports a keras model for serving.\n",
        "  Args:\n",
        "    tf_transform_output: Wrapper around output of tf.Transform.\n",
        "    model: A keras model to export for serving.\n",
        "    output_dir: A directory where the model will be exported to.\n",
        "  \"\"\"\n",
        "\n",
        "  # The layer has to be saved to the model for keras tracking purpases.\n",
        "  \n",
        "  model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "\n",
        "  signatures = {\n",
        "      'serving_default':\n",
        "          _get_tf_examples_serving_signature(model, tf_transform_output),\n",
        "      'transform_features':\n",
        "          _get_transform_features_signature(model, tf_transform_output),\n",
        "  }\n",
        "\n",
        "  model.save(output_dir, save_format='tf', signatures=signatures)\n",
        "\n",
        "\n",
        "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "                                            trainable=False,\n",
        "                                            name=\"universal_sentence_encoder\")\n",
        "\n",
        "char_vectorizer = layers.TextVectorization(max_tokens=NUM_CHAR_TOKENS,  \n",
        "                                    output_sequence_length=output_seq_char_len,\n",
        "                                    standardize=\"lower_and_strip_punctuation\",\n",
        "                                    name=\"char_vectorizer\")\n",
        "\n",
        "\n",
        "char_embed = layers.Embedding(input_dim=NUM_CHAR_TOKENS,\n",
        "                              output_dim=25, \n",
        "                              mask_zero=False, \n",
        "                              name=\"char_embed\")\n",
        "\n",
        "def model_builder():\n",
        "    \n",
        "    token_inputs = layers.Input(shape=[], dtype=\"string\", name=\"token_inputs\")\n",
        "    token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
        "    token_outputs = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
        "    token_model = tf.keras.Model(inputs=token_inputs,\n",
        "                                outputs=token_outputs)\n",
        "\n",
        "    # 2. Char inputs\n",
        "    char_inputs = layers.Input(shape=(1,), dtype=\"string\", name=\"char_inputs\")\n",
        "    char_vectors = char_vectorizer(char_inputs)\n",
        "    char_embeddings = char_embed(char_vectors)\n",
        "    char_bi_lstm = layers.Bidirectional(layers.LSTM(32))(char_embeddings)\n",
        "    char_model = tf.keras.Model(inputs=char_inputs,\n",
        "                                outputs=char_bi_lstm)\n",
        "\n",
        "    # 3. Line numbers inputs\n",
        "    line_number_inputs = layers.Input(shape=(15,), dtype=tf.int32, name=\"line_number_input\")\n",
        "    x = layers.Dense(32, activation=\"relu\")(line_number_inputs)\n",
        "\n",
        "    line_number_model = tf.keras.Model(inputs=line_number_inputs,\n",
        "                                    outputs=x)\n",
        "\n",
        "    # 4. Total lines inputs\n",
        "    total_lines_inputs = layers.Input(shape=(20,), dtype=tf.int32, name=\"total_lines_input\")\n",
        "    y = layers.Dense(32, activation=\"relu\")(total_lines_inputs)\n",
        "    total_line_model = tf.keras.Model(inputs=total_lines_inputs,\n",
        "                                    outputs=y)\n",
        "\n",
        "    # 5. Combine token and char embeddings into a hybrid embedding\n",
        "    combined_embeddings = layers.Concatenate(name=\"token_char_hybrid_embedding\")([token_model.output, \n",
        "                                                                                char_model.output])\n",
        "    z = layers.Dense(256, activation=\"relu\")(combined_embeddings)\n",
        "    z = layers.Dropout(0.5)(z)\n",
        "\n",
        "    # 6. Combine positional embeddings with combined token and char embeddings into a tribrid embedding\n",
        "    z = layers.Concatenate(name=\"token_char_positional_embedding\")([line_number_model.output,\n",
        "                                                                    total_line_model.output,\n",
        "                                                                    z])\n",
        "\n",
        "\n",
        "    # 7. Create output layer\n",
        "    output_layer = layers.Dense(5, activation=\"softmax\", name=\"output_layer\")(z)\n",
        "\n",
        "    # 8. Put together model\n",
        "    model_5 = tf.keras.Model(inputs=[line_number_model.input,\n",
        "                                    total_line_model.input,\n",
        "                                    token_model.input, \n",
        "                                    char_model.input],\n",
        "                            outputs=output_layer)\n",
        "\n",
        "\n",
        "    model_5.compile(loss='categorical_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "    return model_5\n",
        "\n",
        "\n",
        "    \n",
        "def run_fn(fn_args: tfx.components.FnArgs) -> None:\n",
        "    \n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "        log_dir = fn_args.model_run_dir, update_freq='batch'\n",
        "    )\n",
        "    \n",
        "    es = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1, patience=10)\n",
        "    mc = tf.keras.callbacks.ModelCheckpoint(fn_args.serving_model_dir, monitor='val_categorical_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "    \n",
        "\n",
        "    # Load the transform output\n",
        "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
        "    \n",
        "    # Create batches of data\n",
        "    train_set = _input_fn(fn_args.train_files,fn_args.data_accessor, tf_transform_output)\n",
        "    val_set = _input_fn(fn_args.eval_files,fn_args.data_accessor, tf_transform_output)\n",
        "    \n",
        "    \n",
        "    vectorize_dataset = train_set.map(lambda f, l: f['char_inputs']).unbatch()\n",
        "    char_vectorizer.adapt(vectorize_dataset.take(1000))\n",
        "\n",
        "    # Build the model\n",
        "    model = model_builder()\n",
        "\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(train_set,\n",
        "            validation_data = val_set,\n",
        "            callbacks = [tensorboard_callback, es, mc],\n",
        "            steps_per_epoch = 1000, \n",
        "            validation_steps= 1000,\n",
        "            epochs=1)\n",
        "    export_serving_model(tf_transform_output, model, fn_args.serving_model_dir)\n",
        "    "
      ],
      "metadata": {
        "id": "n9a9lAh5-Mw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tfx.components import Trainer \n",
        "from tfx.proto import trainer_pb2 \n",
        "\n",
        "trainer  = Trainer(\n",
        "    module_file=skimlit_trainer_module_file,\n",
        "    examples = transform.outputs['transformed_examples'],\n",
        "    \n",
        "    transform_graph=transform.outputs['transform_graph'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    train_args=trainer_pb2.TrainArgs(splits=['train']),\n",
        "    eval_args=trainer_pb2.EvalArgs(splits=['eval'])\n",
        ")\n",
        "\n",
        "context.run(trainer)"
      ],
      "metadata": {
        "id": "DodEiBX3-M3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_run_artifact_dir = trainer.outputs['model_run'].get()[0].uri\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {model_run_artifact_dir}"
      ],
      "metadata": {
        "id": "8ifbkR7q-M7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tfx.dsl.components.common.resolver import Resolver \n",
        "from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import LatestBlessedModelStrategy \n",
        "from tfx.types import Channel \n",
        "from tfx.types.standard_artifacts import Model, ModelBlessing \n",
        "\n",
        "model_resolver = Resolver(\n",
        "    strategy_class= LatestBlessedModelStrategy,\n",
        "    model = Channel(type=Model),\n",
        "    model_blessing = Channel(type=ModelBlessing)\n",
        ").with_id('Latest_blessed_model_resolver')\n",
        "\n",
        "context.run(model_resolver)\n"
      ],
      "metadata": {
        "id": "nJ4JFBjkk_85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg27Q5HekkTj"
      },
      "outputs": [],
      "source": [
        "import tensorflow_model_analysis as tfma \n",
        "\n",
        "accuracy_threshold = tfma.MetricThreshold(\n",
        "                    value_threshold=tfma.GenericValueThreshold(\n",
        "                        lower_bound={'value':0.5}),\n",
        "                    change_threshold=tfma.GenericChangeThreshold(\n",
        "                        direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
        "                        absolute={'value':0.0001}))\n",
        "\n",
        "eval_config = tfma.EvalConfig(\n",
        "    model_specs=[tfma.ModelSpec(signature_name='serving_default',\n",
        "                                label_key='target_int',\n",
        "                                preprocessing_function_names=['transform_features']\n",
        "    )],\n",
        "    slicing_specs=[tfma.SlicingSpec()],\n",
        "    metrics_specs=[\n",
        "        tfma.MetricsSpec(metrics=[\n",
        "            \n",
        "            tfma.MetricConfig(class_name='ExampleCount'),\n",
        "            # tfma.MetricConfig(class_name='SparseCategoricalAccuracy',\n",
        "            #                  threshold=accuracy_threshold  ),\n",
        "               \n",
        "        ])\n",
        "    ]\n",
        "\n",
        ")\n",
        "\n",
        "from tfx.components import Evaluator\n",
        "\n",
        "evaluator = Evaluator(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    model=trainer.outputs['model'],\n",
        "    baseline_model=model_resolver.outputs['model'],\n",
        "    eval_config=eval_config)\n",
        "\n",
        "context.run(evaluator,enable_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the evaluation results\n",
        "eval_result = evaluator.outputs['evaluation'].get()[0].uri\n",
        "tfma_result = tfma.load_eval_result(eval_result)\n",
        "tfma.view.render_slicing_metrics(tfma_result)"
      ],
      "metadata": {
        "id": "rksoxo3o4_aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CSP1NGlkkXW"
      },
      "outputs": [],
      "source": [
        "tfma.addons.fairness.view.widget_view.render_fairness_indicator(tfma_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mrs2Zx5-kkkg"
      },
      "outputs": [],
      "source": [
        "from tfx.components import Pusher \n",
        "from tfx.proto import pusher_pb2 \n",
        "\n",
        "pusher = Pusher(\n",
        "model=trainer.outputs['model'],\n",
        "model_blessing=evaluator.outputs['blessing'],\n",
        "push_destination=pusher_pb2.PushDestination(\n",
        "    filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "        base_directory='serving_model_dir'))\n",
        "\n",
        ")\n",
        "\n",
        "context.run(pusher)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pusher.outputs['pushed_model'].get()[0].uri)"
      ],
      "metadata": {
        "id": "7B1Lez7Wk2DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_DIR = pusher.outputs['pushed_model'].get()[0].uri"
      ],
      "metadata": {
        "id": "UyVr9qR_H5iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l MODEL_DIR"
      ],
      "metadata": {
        "id": "sVbAFPyyIgsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "# We need sudo prefix if not on a Google Colab.\n",
        "if 'google.colab' not in sys.modules:\n",
        "  SUDO_IF_NEEDED = 'sudo'\n",
        "else:\n",
        "  SUDO_IF_NEEDED = ''"
      ],
      "metadata": {
        "id": "Owx_wKbpkyI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l './serving_model_dir'"
      ],
      "metadata": {
        "id": "oWO87851IsJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ua6Hd7TtkIx4"
      },
      "outputs": [],
      "source": [
        "!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | {SUDO_IF_NEEDED} tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | {SUDO_IF_NEEDED} apt-key add -\n",
        "!{SUDO_IF_NEEDED} apt update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIzQJFsGkJG1"
      },
      "outputs": [],
      "source": [
        "!{SUDO_IF_NEEDED} apt-get install tensorflow-model-server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnbLqb3aTLQc"
      },
      "outputs": [],
      "source": [
        "os.environ[\"MODEL_DIR\"] = MODEL_DIR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup tensorflow_model_server \\\n",
        "  --rest_api_port=8501 \\\n",
        "  --model_name=fashion_model \\\n",
        "  --model_base_path=\"${MODEL_DIR}\" >server.log 2>&1"
      ],
      "metadata": {
        "id": "gtJEykRMH2-r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}